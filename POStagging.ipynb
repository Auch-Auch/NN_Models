{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "POStagging.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "P59NYU98GCb9"
      },
      "source": [
        "!pip3 -qq install torch==0.4.1\n",
        "!pip3 -qq install bokeh==0.13.0\n",
        "!pip3 -qq install gensim==3.6.0\n",
        "!pip3 -qq install nltk\n",
        "!pip3 -qq install scikit-learn==0.20.2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8sVtGHmA9aBM"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    from torch.cuda import FloatTensor, LongTensor\n",
        "else:\n",
        "    from torch import FloatTensor, LongTensor\n",
        "\n",
        "np.random.seed(42)"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TiA2dGmgF1rW",
        "outputId": "ea4da2f3-f61c-4970-89dc-544c9f2e1527",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        }
      },
      "source": [
        "import nltk\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "nltk.download('brown')\n",
        "nltk.download('universal_tagset')\n",
        "\n",
        "data = nltk.corpus.brown.tagged_sents(tagset='universal')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/brown.zip.\n",
            "[nltk_data] Downloading package universal_tagset to /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/universal_tagset.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xTai8Ta0lgwL"
      },
      "source": [
        "train_data, test_data = train_test_split(data, test_size=0.25, random_state=42)\n",
        "train_data, val_data = train_test_split(train_data, test_size=0.15, random_state=42)\n",
        "\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pCjwwDs6Zq9x",
        "outputId": "7ac22894-9a34-40a3-88d6-dbfbba40aa1d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "words = {word for sample in train_data for word, tag in sample}\n",
        "word2ind = {word: ind + 1 for ind, word in enumerate(words)}\n",
        "word2ind['<pad>'] = 0\n",
        "\n",
        "tags = {tag for sample in train_data for word, tag in sample}\n",
        "tag2ind = {tag: ind + 1 for ind, tag in enumerate(tags)}\n",
        "tag2ind['<pad>'] = 0\n",
        "\n",
        "print('Unique words in train = {}. Tags = {}'.format(len(word2ind), tags))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Unique words in train = 45441. Tags = {'X', 'ADV', 'ADJ', 'PRT', 'ADP', '.', 'PRON', 'DET', 'NOUN', 'NUM', 'VERB', 'CONJ'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "URC1B2nvPGFt",
        "outputId": "66c89954-3cc4-439e-cd9b-db3e623fd9f4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 320
        }
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "from collections import Counter\n",
        "\n",
        "tag_distribution = Counter(tag for sample in train_data for _, tag in sample)\n",
        "tag_distribution = [tag_distribution[tag] for tag in tags]\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "\n",
        "bar_width = 0.35\n",
        "plt.bar(np.arange(len(tags)), tag_distribution, bar_width, align='center', alpha=0.5)\n",
        "plt.xticks(np.arange(len(tags)), tags)\n",
        "    \n",
        "plt.show()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RtRbz1SwgEqc"
      },
      "source": [
        "def convert_data(data, word2ind, tag2ind):\n",
        "    X = [[word2ind.get(word, 0) for word, _ in sample] for sample in data]\n",
        "    y = [[tag2ind[tag] for _, tag in sample] for sample in data]\n",
        "    \n",
        "    return X, y\n",
        "\n",
        "X_train, y_train = convert_data(train_data, word2ind, tag2ind)\n",
        "X_val, y_val = convert_data(val_data, word2ind, tag2ind)\n",
        "X_test, y_test = convert_data(test_data, word2ind, tag2ind)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DhsTKZalfih6"
      },
      "source": [
        "def iterate_batches(data, batch_size):\n",
        "    X, y = data\n",
        "    n_samples = len(X)\n",
        "\n",
        "    indices = np.arange(n_samples)\n",
        "    np.random.shuffle(indices)\n",
        "    \n",
        "    for start in range(0, n_samples, batch_size):\n",
        "        end = min(start + batch_size, n_samples)\n",
        "        \n",
        "        batch_indices = indices[start:end]\n",
        "        \n",
        "        max_sent_len = max(len(X[ind]) for ind in batch_indices)\n",
        "        X_batch = np.zeros((max_sent_len, len(batch_indices)))\n",
        "        y_batch = np.zeros((max_sent_len, len(batch_indices)))\n",
        "        \n",
        "        for batch_ind, sample_ind in enumerate(batch_indices):\n",
        "            X_batch[:len(X[sample_ind]), batch_ind] = X[sample_ind]\n",
        "            y_batch[:len(y[sample_ind]), batch_ind] = y[sample_ind]\n",
        "            \n",
        "        yield X_batch, y_batch"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l4XsRII5kW5x",
        "outputId": "f71ff165-860f-473d-f370-25faaca8e722",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "X_batch, y_batch = next(iterate_batches((X_train, y_train), 2))\n",
        "\n",
        "X_batch.shape, y_batch.shape"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((23, 2), (23, 2))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WVEHju54d68T"
      },
      "source": [
        "class LSTMTagger(nn.Module):\n",
        "    def __init__(self, vocab_size, tagset_size, word_emb_dim=100, lstm_hidden_dim=128, lstm_layers_count=1):\n",
        "        super().__init__()\n",
        "        self.hidden_size = lstm_hidden_dim\n",
        "        self.word_embeddings = nn.Embedding(vocab_size, word_emb_dim)\n",
        "        self.lstm = nn.LSTM(word_emb_dim, lstm_hidden_dim, lstm_layers_count)\n",
        "        self.hidden2tag = nn.Linear(lstm_hidden_dim, tagset_size)\n",
        "        \n",
        "\n",
        "    def forward(self, sentence):\n",
        "        embeds = self.word_embeddings(sentence)\n",
        "        lstm_out, _ = self.lstm(embeds)\n",
        "        tag_space = self.hidden2tag(lstm_out)\n",
        "        tag_scores = F.log_softmax(tag_space)\n",
        "        return tag_scores\n",
        "\n",
        "\n",
        "        "
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jbrxsZ2mehWB",
        "outputId": "d2a622e7-eaee-451d-910b-d68a83fcabfc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "model = LSTMTagger(\n",
        "    vocab_size=len(word2ind),\n",
        "    tagset_size=len(tag2ind)\n",
        ")\n",
        "\n",
        "X_batch, y_batch = torch.LongTensor(X_batch), torch.LongTensor(y_batch)\n",
        "\n",
        "logits = model(X_batch)\n",
        "\n",
        "\n"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:14: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  \n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GMUyUm1hgpe3"
      },
      "source": [
        "criterion = nn.CrossEntropyLoss(ignore_index=0)\n"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FprPQ0gllo7b"
      },
      "source": [
        "import math\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "def do_epoch(model, criterion, data, batch_size, scheduler, optimizer=None, name=None):\n",
        "    epoch_loss = 0\n",
        "    correct_count = 0\n",
        "    sum_count = 0\n",
        "    is_train = not optimizer is None\n",
        "    name = name or ''\n",
        "    model.train(is_train)\n",
        "    \n",
        "    batches_count = math.ceil(len(data[0]) / batch_size)\n",
        "    acc = 0\n",
        "    with torch.autograd.set_grad_enabled(is_train):\n",
        "        with tqdm(total=batches_count) as progress_bar:\n",
        "            for i, (X_batch, y_batch) in enumerate(iterate_batches(data, batch_size)):\n",
        "                X_batch, y_batch = LongTensor(X_batch), LongTensor(y_batch)\n",
        "                logits = model(X_batch)\n",
        "                loss = criterion(logits.view(-1, 13), y_batch.view(-1))\n",
        "                \n",
        "                epoch_loss += loss.item()\n",
        "\n",
        "                if optimizer:\n",
        "                    optimizer.zero_grad()\n",
        "                    loss.backward()\n",
        "                    optimizer.step()\n",
        "    \n",
        "                _, indices = torch.max(logits, 2)\n",
        "                cur_acc = torch.mean(torch.tensor(y_batch == indices, dtype=torch.float))\n",
        "                acc += cur_acc\n",
        "                progress_bar.update()\n",
        "                progress_bar.set_description('{:>5s} Loss = {:.5f}, Accuracy = {:.5%}'.format(\n",
        "                    name, loss.item(), cur_acc)\n",
        "                )\n",
        "            scheduler.step()\n",
        "            acc = acc / batches_count\n",
        "            progress_bar.set_description('{:>5s} Loss = {:.5f}, Accuracy = {:.5%}'.format(\n",
        "                name, epoch_loss / batches_count, acc)\n",
        "            )\n",
        "\n",
        "    return epoch_loss / batches_count, acc\n",
        "\n",
        "\n",
        "def fit(model, criterion, optimizer, scheduler, train_data, epochs_count=1, batch_size=32,\n",
        "        val_data=None, val_batch_size=None):\n",
        "        \n",
        "    if not val_data is None and val_batch_size is None:\n",
        "        val_batch_size = batch_size\n",
        "        \n",
        "    for epoch in range(epochs_count):\n",
        "        name_prefix = '[{} / {}] '.format(epoch + 1, epochs_count)\n",
        "        train_loss, train_acc = do_epoch(model, criterion, train_data, batch_size, scheduler, optimizer, name_prefix + 'Train:')\n",
        "        \n",
        "        if not val_data is None:\n",
        "            val_loss, val_acc = do_epoch(model, criterion, val_data, val_batch_size, scheduler, None, name_prefix + '  Val:')"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pqfbeh1ltEYa",
        "outputId": "7d609ebc-1e38-40dd-eeb8-9892e08f6af7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model = LSTMTagger(\n",
        "    vocab_size=len(word2ind),\n",
        "    tagset_size=len(tag2ind)\n",
        ").cuda()\n",
        "\n",
        "criterion = nn.CrossEntropyLoss().cuda()\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
        "fit(model, criterion, optimizer, scheduler, train_data=(X_train, y_train), epochs_count=50,\n",
        "    batch_size=64, val_data=(X_val, y_val), val_batch_size=512)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  0%|          | 0/572 [00:00<?, ?it/s]/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:14: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  \n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:30: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "[1 / 50] Train: Loss = 0.43529, Accuracy = 89.82725%: 100%|██████████| 572/572 [00:04<00:00, 118.68it/s]\n",
            "[1 / 50]   Val: Loss = 0.17340, Accuracy = 95.41552%: 100%|██████████| 13/13 [00:00<00:00, 80.65it/s]\n",
            "[2 / 50] Train: Loss = 0.16201, Accuracy = 95.08708%: 100%|██████████| 572/572 [00:04<00:00, 121.36it/s]\n",
            "[2 / 50]   Val: Loss = 0.12998, Accuracy = 96.69463%: 100%|██████████| 13/13 [00:00<00:00, 83.65it/s]\n",
            "[3 / 50] Train: Loss = 0.11960, Accuracy = 96.21589%: 100%|██████████| 572/572 [00:04<00:00, 123.03it/s]\n",
            "[3 / 50]   Val: Loss = 0.11685, Accuracy = 97.13004%: 100%|██████████| 13/13 [00:00<00:00, 82.47it/s]\n",
            "[4 / 50] Train: Loss = 0.09702, Accuracy = 96.85828%: 100%|██████████| 572/572 [00:04<00:00, 122.42it/s]\n",
            "[4 / 50]   Val: Loss = 0.11072, Accuracy = 97.33421%: 100%|██████████| 13/13 [00:00<00:00, 82.03it/s]\n",
            "[5 / 50] Train: Loss = 0.08476, Accuracy = 97.19788%: 100%|██████████| 572/572 [00:04<00:00, 122.01it/s]\n",
            "[5 / 50]   Val: Loss = 0.11355, Accuracy = 97.39811%: 100%|██████████| 13/13 [00:00<00:00, 86.02it/s]\n",
            "[6 / 50] Train: Loss = 0.07495, Accuracy = 97.48998%: 100%|██████████| 572/572 [00:04<00:00, 123.37it/s]\n",
            "[6 / 50]   Val: Loss = 0.10971, Accuracy = 97.53780%: 100%|██████████| 13/13 [00:00<00:00, 82.13it/s]\n",
            "[7 / 50] Train: Loss = 0.07420, Accuracy = 97.50810%: 100%|██████████| 572/572 [00:04<00:00, 122.79it/s]\n",
            "[7 / 50]   Val: Loss = 0.11124, Accuracy = 97.50673%: 100%|██████████| 13/13 [00:00<00:00, 78.71it/s]\n",
            "[8 / 50] Train: Loss = 0.07285, Accuracy = 97.54374%: 100%|██████████| 572/572 [00:04<00:00, 122.77it/s]\n",
            "[8 / 50]   Val: Loss = 0.11015, Accuracy = 97.55266%: 100%|██████████| 13/13 [00:00<00:00, 81.94it/s]\n",
            "[9 / 50] Train: Loss = 0.07161, Accuracy = 97.57700%: 100%|██████████| 572/572 [00:04<00:00, 122.14it/s]\n",
            "[9 / 50]   Val: Loss = 0.12115, Accuracy = 97.34269%: 100%|██████████| 13/13 [00:00<00:00, 85.30it/s]\n",
            "[10 / 50] Train: Loss = 0.07096, Accuracy = 97.58976%: 100%|██████████| 572/572 [00:04<00:00, 122.83it/s]\n",
            "[10 / 50]   Val: Loss = 0.12308, Accuracy = 97.31070%: 100%|██████████| 13/13 [00:00<00:00, 84.50it/s]\n",
            "[11 / 50] Train: Loss = 0.06996, Accuracy = 97.61582%: 100%|██████████| 572/572 [00:04<00:00, 121.81it/s]\n",
            "[11 / 50]   Val: Loss = 0.10737, Accuracy = 97.64668%: 100%|██████████| 13/13 [00:00<00:00, 78.06it/s]\n",
            "[12 / 50] Train: Loss = 0.07032, Accuracy = 97.60290%: 100%|██████████| 572/572 [00:04<00:00, 122.55it/s]\n",
            "[12 / 50]   Val: Loss = 0.11640, Accuracy = 97.45864%: 100%|██████████| 13/13 [00:00<00:00, 80.87it/s]\n",
            "[13 / 50] Train: Loss = 0.06977, Accuracy = 97.62102%: 100%|██████████| 572/572 [00:04<00:00, 121.46it/s]\n",
            "[13 / 50]   Val: Loss = 0.10660, Accuracy = 97.67250%: 100%|██████████| 13/13 [00:00<00:00, 80.08it/s]\n",
            "[14 / 50] Train: Loss = 0.06980, Accuracy = 97.62294%: 100%|██████████| 572/572 [00:04<00:00, 121.95it/s]\n",
            "[14 / 50]   Val: Loss = 0.10572, Accuracy = 97.69055%: 100%|██████████| 13/13 [00:00<00:00, 80.29it/s]\n",
            "[15 / 50] Train: Loss = 0.06947, Accuracy = 97.62854%: 100%|██████████| 572/572 [00:04<00:00, 121.90it/s]\n",
            "[15 / 50]   Val: Loss = 0.11404, Accuracy = 97.51853%: 100%|██████████| 13/13 [00:00<00:00, 84.01it/s]\n",
            "[16 / 50] Train: Loss = 0.06955, Accuracy = 97.62635%: 100%|██████████| 572/572 [00:04<00:00, 121.72it/s]\n",
            "[16 / 50]   Val: Loss = 0.10940, Accuracy = 97.60682%: 100%|██████████| 13/13 [00:00<00:00, 81.41it/s]\n",
            "[17 / 50] Train: Loss = 0.07003, Accuracy = 97.61044%: 100%|██████████| 572/572 [00:04<00:00, 121.20it/s]\n",
            "[17 / 50]   Val: Loss = 0.11060, Accuracy = 97.59387%: 100%|██████████| 13/13 [00:00<00:00, 82.87it/s]\n",
            "[18 / 50] Train: Loss = 0.06961, Accuracy = 97.62803%: 100%|██████████| 572/572 [00:04<00:00, 120.23it/s]\n",
            "[18 / 50]   Val: Loss = 0.12217, Accuracy = 97.35942%: 100%|██████████| 13/13 [00:00<00:00, 83.06it/s]\n",
            "[19 / 50] Train: Loss = 0.06953, Accuracy = 97.62710%: 100%|██████████| 572/572 [00:04<00:00, 122.90it/s]\n",
            "[19 / 50]   Val: Loss = 0.11453, Accuracy = 97.51088%: 100%|██████████| 13/13 [00:00<00:00, 80.60it/s]\n",
            "[20 / 50] Train: Loss = 0.06953, Accuracy = 97.62657%: 100%|██████████| 572/572 [00:04<00:00, 121.63it/s]\n",
            "[20 / 50]   Val: Loss = 0.11756, Accuracy = 97.44351%: 100%|██████████| 13/13 [00:00<00:00, 86.67it/s]\n",
            "[21 / 50] Train: Loss = 0.06966, Accuracy = 97.62310%: 100%|██████████| 572/572 [00:04<00:00, 119.88it/s]\n",
            "[21 / 50]   Val: Loss = 0.11175, Accuracy = 97.57140%: 100%|██████████| 13/13 [00:00<00:00, 81.12it/s]\n",
            "[22 / 50] Train: Loss = 0.06972, Accuracy = 97.61986%: 100%|██████████| 572/572 [00:04<00:00, 120.49it/s]\n",
            "[22 / 50]   Val: Loss = 0.12002, Accuracy = 97.39271%: 100%|██████████| 13/13 [00:00<00:00, 81.92it/s]\n",
            "[23 / 50] Train: Loss = 0.06915, Accuracy = 97.63964%: 100%|██████████| 572/572 [00:04<00:00, 120.40it/s]\n",
            "[23 / 50]   Val: Loss = 0.11250, Accuracy = 97.54719%: 100%|██████████| 13/13 [00:00<00:00, 82.52it/s]\n",
            "[24 / 50] Train: Loss = 0.06907, Accuracy = 97.64583%: 100%|██████████| 572/572 [00:04<00:00, 119.21it/s]\n",
            "[24 / 50]   Val: Loss = 0.12325, Accuracy = 97.31259%: 100%|██████████| 13/13 [00:00<00:00, 86.60it/s]\n",
            "[25 / 50] Train: Loss = 0.06945, Accuracy = 97.62866%: 100%|██████████| 572/572 [00:04<00:00, 119.82it/s]\n",
            "[25 / 50]   Val: Loss = 0.10969, Accuracy = 97.60625%: 100%|██████████| 13/13 [00:00<00:00, 72.49it/s]\n",
            "[26 / 50] Train: Loss = 0.06922, Accuracy = 97.63659%: 100%|██████████| 572/572 [00:04<00:00, 117.84it/s]\n",
            "[26 / 50]   Val: Loss = 0.11265, Accuracy = 97.54984%: 100%|██████████| 13/13 [00:00<00:00, 82.51it/s]\n",
            "[27 / 50] Train: Loss = 0.06950, Accuracy = 97.62710%: 100%|██████████| 572/572 [00:04<00:00, 117.96it/s]\n",
            "[27 / 50]   Val: Loss = 0.11577, Accuracy = 97.47730%: 100%|██████████| 13/13 [00:00<00:00, 78.87it/s]\n",
            "[28 / 50] Train: Loss = 0.06942, Accuracy = 97.63306%: 100%|██████████| 572/572 [00:04<00:00, 117.03it/s]\n",
            "[28 / 50]   Val: Loss = 0.10844, Accuracy = 97.62623%: 100%|██████████| 13/13 [00:00<00:00, 77.42it/s]\n",
            "[29 / 50] Train: Loss = 0.06896, Accuracy = 97.64771%: 100%|██████████| 572/572 [00:04<00:00, 117.47it/s]\n",
            "[29 / 50]   Val: Loss = 0.11434, Accuracy = 97.50566%: 100%|██████████| 13/13 [00:00<00:00, 82.62it/s]\n",
            "[30 / 50] Train: Loss = 0.06961, Accuracy = 97.62430%: 100%|██████████| 572/572 [00:04<00:00, 118.27it/s]\n",
            "[30 / 50]   Val: Loss = 0.11462, Accuracy = 97.49801%: 100%|██████████| 13/13 [00:00<00:00, 79.18it/s]\n",
            "[31 / 50] Train: Loss = 0.06911, Accuracy = 97.64153%: 100%|██████████| 572/572 [00:04<00:00, 117.26it/s]\n",
            "[31 / 50]   Val: Loss = 0.11347, Accuracy = 97.52115%: 100%|██████████| 13/13 [00:00<00:00, 77.06it/s]\n",
            "[32 / 50] Train: Loss = 0.06963, Accuracy = 97.62428%: 100%|██████████| 572/572 [00:04<00:00, 117.25it/s]\n",
            "[32 / 50]   Val: Loss = 0.11068, Accuracy = 97.58520%: 100%|██████████| 13/13 [00:00<00:00, 81.26it/s]\n",
            "[33 / 50] Train: Loss = 0.06947, Accuracy = 97.63091%: 100%|██████████| 572/572 [00:04<00:00, 117.27it/s]\n",
            "[33 / 50]   Val: Loss = 0.11396, Accuracy = 97.50690%: 100%|██████████| 13/13 [00:00<00:00, 78.81it/s]\n",
            "[34 / 50] Train: Loss = 0.06942, Accuracy = 97.62906%: 100%|██████████| 572/572 [00:04<00:00, 118.59it/s]\n",
            "[34 / 50]   Val: Loss = 0.11832, Accuracy = 97.43127%: 100%|██████████| 13/13 [00:00<00:00, 73.33it/s]\n",
            "[35 / 50] Train: Loss = 0.06953, Accuracy = 97.62861%: 100%|██████████| 572/572 [00:04<00:00, 114.67it/s]\n",
            "[35 / 50]   Val: Loss = 0.10642, Accuracy = 97.68073%: 100%|██████████| 13/13 [00:00<00:00, 78.77it/s]\n",
            "[36 / 50] Train: Loss = 0.06909, Accuracy = 97.64079%: 100%|██████████| 572/572 [00:05<00:00, 113.36it/s]\n",
            "[36 / 50]   Val: Loss = 0.12104, Accuracy = 97.36931%: 100%|██████████| 13/13 [00:00<00:00, 77.90it/s]\n",
            "[37 / 50] Train: Loss = 0.06999, Accuracy = 97.61159%: 100%|██████████| 572/572 [00:05<00:00, 113.84it/s]\n",
            "[37 / 50]   Val: Loss = 0.11794, Accuracy = 97.42284%: 100%|██████████| 13/13 [00:00<00:00, 85.31it/s]\n",
            "[38 / 50] Train: Loss = 0.06965, Accuracy = 97.62385%: 100%|██████████| 572/572 [00:04<00:00, 116.24it/s]\n",
            "[38 / 50]   Val: Loss = 0.10600, Accuracy = 97.68674%: 100%|██████████| 13/13 [00:00<00:00, 79.56it/s]\n",
            "[39 / 50] Train: Loss = 0.06965, Accuracy = 97.62058%: 100%|██████████| 572/572 [00:04<00:00, 116.53it/s]\n",
            "[39 / 50]   Val: Loss = 0.12532, Accuracy = 97.26929%: 100%|██████████| 13/13 [00:00<00:00, 83.52it/s]\n",
            "[40 / 50] Train: Loss = 0.06952, Accuracy = 97.62988%: 100%|██████████| 572/572 [00:04<00:00, 117.21it/s]\n",
            "[40 / 50]   Val: Loss = 0.11750, Accuracy = 97.45036%: 100%|██████████| 13/13 [00:00<00:00, 80.29it/s]\n",
            "[41 / 50] Train: Loss = 0.06938, Accuracy = 97.63117%: 100%|██████████| 572/572 [00:04<00:00, 115.95it/s]\n",
            "[41 / 50]   Val: Loss = 0.11443, Accuracy = 97.50955%: 100%|██████████| 13/13 [00:00<00:00, 80.01it/s]\n",
            "[42 / 50] Train: Loss = 0.06948, Accuracy = 97.62888%: 100%|██████████| 572/572 [00:04<00:00, 116.47it/s]\n",
            "[42 / 50]   Val: Loss = 0.12786, Accuracy = 97.23101%: 100%|██████████| 13/13 [00:00<00:00, 82.59it/s]\n",
            "[43 / 50] Train: Loss = 0.06933, Accuracy = 97.63181%: 100%|██████████| 572/572 [00:04<00:00, 115.04it/s]\n",
            "[43 / 50]   Val: Loss = 0.11870, Accuracy = 97.41090%: 100%|██████████| 13/13 [00:00<00:00, 82.60it/s]\n",
            "[44 / 50] Train: Loss = 0.06931, Accuracy = 97.63514%: 100%|██████████| 572/572 [00:05<00:00, 113.74it/s]\n",
            "[44 / 50]   Val: Loss = 0.11692, Accuracy = 97.45416%: 100%|██████████| 13/13 [00:00<00:00, 79.70it/s]\n",
            "[45 / 50] Train: Loss = 0.06959, Accuracy = 97.62385%: 100%|██████████| 572/572 [00:05<00:00, 114.14it/s]\n",
            "[45 / 50]   Val: Loss = 0.11487, Accuracy = 97.50013%: 100%|██████████| 13/13 [00:00<00:00, 80.95it/s]\n",
            "[46 / 50] Train: Loss = 0.06938, Accuracy = 97.63224%: 100%|██████████| 572/572 [00:04<00:00, 114.47it/s]\n",
            "[46 / 50]   Val: Loss = 0.11654, Accuracy = 97.46444%: 100%|██████████| 13/13 [00:00<00:00, 76.08it/s]\n",
            "[47 / 50] Train: Loss = 0.06977, Accuracy = 97.61832%: 100%|██████████| 572/572 [00:04<00:00, 114.92it/s]\n",
            "[47 / 50]   Val: Loss = 0.11238, Accuracy = 97.55196%: 100%|██████████| 13/13 [00:00<00:00, 80.21it/s]\n",
            "[48 / 50] Train: Loss = 0.06931, Accuracy = 97.63330%: 100%|██████████| 572/572 [00:05<00:00, 111.72it/s]\n",
            "[48 / 50]   Val: Loss = 0.11692, Accuracy = 97.45234%: 100%|██████████| 13/13 [00:00<00:00, 78.44it/s]\n",
            "[49 / 50] Train: Loss = 0.06922, Accuracy = 97.63685%: 100%|██████████| 572/572 [00:04<00:00, 115.47it/s]\n",
            "[49 / 50]   Val: Loss = 0.11599, Accuracy = 97.48263%: 100%|██████████| 13/13 [00:00<00:00, 84.67it/s]\n",
            "[50 / 50] Train: Loss = 0.06913, Accuracy = 97.63898%: 100%|██████████| 572/572 [00:05<00:00, 113.56it/s]\n",
            "[50 / 50]   Val: Loss = 0.11879, Accuracy = 97.40611%: 100%|██████████| 13/13 [00:00<00:00, 79.02it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "98wr38_rw55D",
        "outputId": "eec912c3-ce6c-47fe-9aef-403d4a86723b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 128
        }
      },
      "source": [
        "\n",
        "acc = 0\n",
        "counter = 0\n",
        "for i, (X_batch, y_batch) in enumerate(iterate_batches((X_test, y_test), 32)):\n",
        "    X, y = LongTensor(X_batch), LongTensor(y_batch)\n",
        "    logits = model(X)\n",
        "    _, indices = torch.max(logits, 2)\n",
        "    cur_acc = torch.mean(torch.tensor(y == indices, dtype=torch.float))\n",
        "    acc += cur_acc\n",
        "    counter = i\n",
        "print(\"Final acc: \", acc.item() / counter)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:14: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  \n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Final acc:  0.9572377727602419\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uZpY_Q1xZ18h",
        "outputId": "017966f7-ad06-4907-caed-2cde2bad1704",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 92
        }
      },
      "source": [
        "import gensim.downloader as api\n",
        "\n",
        "w2v_model = api.load('glove-wiki-gigaword-100')"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[================================================--] 96.0% 123.0/128.1MB downloaded\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:252: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
            "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VsCstxiO03oT",
        "outputId": "c7c1a582-364f-4edb-eae5-8296d414000c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 181
        }
      },
      "source": [
        "known_count = 0\n",
        "embeddings = np.zeros((len(word2ind), w2v_model.vectors.shape[1]))\n",
        "for word, ind in word2ind.items():\n",
        "    word = word.lower()\n",
        "    if word in w2v_model.vocab:\n",
        "        embeddings[ind] = w2v_model.get_vector(word)\n",
        "        known_count += 1\n",
        "        \n",
        "print('Know {} out of {} word embeddings'.format(known_count, len(word2ind)))\n",
        "torch.tensor(embeddings)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Know 38736 out of 45441 word embeddings\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "        [ 0.0924,  0.0161, -0.5831,  ..., -0.0514,  0.2286, -1.7341],\n",
              "        [-0.3224, -0.0658, -0.3195,  ...,  0.3234, -0.0442,  0.0700],\n",
              "        ...,\n",
              "        [-0.1556,  0.5396, -0.4734,  ..., -0.0702,  0.0244,  0.0887],\n",
              "        [ 0.6666,  0.0853,  0.5238,  ..., -0.5173,  0.3076, -0.4327],\n",
              "        [ 0.6431,  1.0674, -0.9012,  ..., -0.2400,  0.4884,  0.2977]],\n",
              "       dtype=torch.float64)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LxaRBpQd0pat"
      },
      "source": [
        "class LSTMTaggerWithPretrainedEmbs(nn.Module):\n",
        "    def __init__(self, embeddings, tagset_size, lstm_hidden_dim=64, lstm_layers_count=1):\n",
        "        super().__init__()\n",
        "        self.hidden_size = lstm_hidden_dim\n",
        "        self.word_embeddings = nn.Embedding.from_pretrained(embeddings)\n",
        "        self.lstm = nn.LSTM(100, lstm_hidden_dim, 1, bidirectional=True)\n",
        "        self.hidden2tag = nn.Linear(lstm_hidden_dim*2, tagset_size)\n",
        "        \n",
        "\n",
        "    def forward(self, sentence):\n",
        "        embeds = self.word_embeddings(sentence)\n",
        "        lstm_out, _ = self.lstm(embeds)\n",
        "        tag_space = self.hidden2tag(lstm_out)\n",
        "        tag_scores = F.log_softmax(tag_space)\n",
        "        return tag_scores"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EBtI6BDE-Fc7",
        "outputId": "69467df0-734c-4d6e-94b5-e97c466949f8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model = LSTMTaggerWithPretrainedEmbs(\n",
        "    embeddings=torch.tensor(embeddings).float(),\n",
        "    tagset_size=len(tag2ind)\n",
        ").cuda()\n",
        "\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=100, gamma=0.1)\n",
        "fit(model, criterion, optimizer, scheduler, train_data=(X_train, y_train), epochs_count=50,\n",
        "    batch_size=64, val_data=(X_val, y_val), val_batch_size=512)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  0%|          | 0/572 [00:00<?, ?it/s]/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:14: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  \n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:30: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "[1 / 50] Train: Loss = 0.59277, Accuracy = 35.02402%: 100%|██████████| 572/572 [00:05<00:00, 113.55it/s]\n",
            "[1 / 50]   Val: Loss = 0.26017, Accuracy = 24.93419%: 100%|██████████| 13/13 [00:00<00:00, 84.02it/s]\n",
            "[2 / 50] Train: Loss = 0.19945, Accuracy = 34.97768%: 100%|██████████| 572/572 [00:05<00:00, 113.75it/s]\n",
            "[2 / 50]   Val: Loss = 0.17916, Accuracy = 26.02004%: 100%|██████████| 13/13 [00:00<00:00, 83.61it/s]\n",
            "[3 / 50] Train: Loss = 0.14334, Accuracy = 39.66945%: 100%|██████████| 572/572 [00:04<00:00, 115.24it/s]\n",
            "[3 / 50]   Val: Loss = 0.14882, Accuracy = 42.06076%: 100%|██████████| 13/13 [00:00<00:00, 83.52it/s]\n",
            "[4 / 50] Train: Loss = 0.11703, Accuracy = 45.97764%: 100%|██████████| 572/572 [00:05<00:00, 114.38it/s]\n",
            "[4 / 50]   Val: Loss = 0.12828, Accuracy = 53.54812%: 100%|██████████| 13/13 [00:00<00:00, 85.15it/s]\n",
            "[5 / 50] Train: Loss = 0.10107, Accuracy = 50.90868%: 100%|██████████| 572/572 [00:05<00:00, 113.22it/s]\n",
            "[5 / 50]   Val: Loss = 0.11836, Accuracy = 59.33539%: 100%|██████████| 13/13 [00:00<00:00, 85.86it/s]\n",
            "[6 / 50] Train: Loss = 0.09069, Accuracy = 56.54756%: 100%|██████████| 572/572 [00:04<00:00, 114.61it/s]\n",
            "[6 / 50]   Val: Loss = 0.11223, Accuracy = 64.01370%: 100%|██████████| 13/13 [00:00<00:00, 88.38it/s]\n",
            "[7 / 50] Train: Loss = 0.08278, Accuracy = 58.03439%: 100%|██████████| 572/572 [00:05<00:00, 113.66it/s]\n",
            "[7 / 50]   Val: Loss = 0.10832, Accuracy = 67.97167%: 100%|██████████| 13/13 [00:00<00:00, 82.58it/s]\n",
            "[8 / 50] Train: Loss = 0.07667, Accuracy = 57.35151%: 100%|██████████| 572/572 [00:05<00:00, 113.31it/s]\n",
            "[8 / 50]   Val: Loss = 0.10386, Accuracy = 65.36016%: 100%|██████████| 13/13 [00:00<00:00, 82.96it/s]\n",
            "[9 / 50] Train: Loss = 0.07162, Accuracy = 55.81260%: 100%|██████████| 572/572 [00:05<00:00, 112.68it/s]\n",
            "[9 / 50]   Val: Loss = 0.10237, Accuracy = 63.04685%: 100%|██████████| 13/13 [00:00<00:00, 86.44it/s]\n",
            "[10 / 50] Train: Loss = 0.06738, Accuracy = 54.34269%: 100%|██████████| 572/572 [00:05<00:00, 114.26it/s]\n",
            "[10 / 50]   Val: Loss = 0.10196, Accuracy = 58.90995%: 100%|██████████| 13/13 [00:00<00:00, 85.28it/s]\n",
            "[11 / 50] Train: Loss = 0.06384, Accuracy = 52.06422%: 100%|██████████| 572/572 [00:05<00:00, 112.10it/s]\n",
            "[11 / 50]   Val: Loss = 0.09771, Accuracy = 53.22223%: 100%|██████████| 13/13 [00:00<00:00, 83.06it/s]\n",
            "[12 / 50] Train: Loss = 0.06078, Accuracy = 49.58385%: 100%|██████████| 572/572 [00:05<00:00, 112.17it/s]\n",
            "[12 / 50]   Val: Loss = 0.09874, Accuracy = 34.79195%: 100%|██████████| 13/13 [00:00<00:00, 86.88it/s]\n",
            "[13 / 50] Train: Loss = 0.05781, Accuracy = 41.02664%: 100%|██████████| 572/572 [00:05<00:00, 111.04it/s]\n",
            "[13 / 50]   Val: Loss = 0.09829, Accuracy = 39.56103%: 100%|██████████| 13/13 [00:00<00:00, 83.98it/s]\n",
            "[14 / 50] Train: Loss = 0.05552, Accuracy = 38.58550%: 100%|██████████| 572/572 [00:05<00:00, 112.56it/s]\n",
            "[14 / 50]   Val: Loss = 0.09640, Accuracy = 23.68873%: 100%|██████████| 13/13 [00:00<00:00, 85.74it/s]\n",
            "[15 / 50] Train: Loss = 0.05295, Accuracy = 37.50046%: 100%|██████████| 572/572 [00:05<00:00, 110.32it/s]\n",
            "[15 / 50]   Val: Loss = 0.09984, Accuracy = 39.56461%: 100%|██████████| 13/13 [00:00<00:00, 82.79it/s]\n",
            "[16 / 50] Train: Loss = 0.05078, Accuracy = 35.96528%: 100%|██████████| 572/572 [00:05<00:00, 113.11it/s]\n",
            "[16 / 50]   Val: Loss = 0.09551, Accuracy = 31.14381%: 100%|██████████| 13/13 [00:00<00:00, 84.91it/s]\n",
            "[17 / 50] Train: Loss = 0.04886, Accuracy = 35.39443%: 100%|██████████| 572/572 [00:05<00:00, 111.29it/s]\n",
            "[17 / 50]   Val: Loss = 0.09845, Accuracy = 22.98408%: 100%|██████████| 13/13 [00:00<00:00, 86.21it/s]\n",
            "[18 / 50] Train: Loss = 0.04704, Accuracy = 35.43617%: 100%|██████████| 572/572 [00:05<00:00, 110.89it/s]\n",
            "[18 / 50]   Val: Loss = 0.09591, Accuracy = 25.11387%: 100%|██████████| 13/13 [00:00<00:00, 83.39it/s]\n",
            "[19 / 50] Train: Loss = 0.04495, Accuracy = 35.36279%: 100%|██████████| 572/572 [00:05<00:00, 110.81it/s]\n",
            "[19 / 50]   Val: Loss = 0.09730, Accuracy = 22.57514%: 100%|██████████| 13/13 [00:00<00:00, 81.47it/s]\n",
            "[20 / 50] Train: Loss = 0.04349, Accuracy = 33.74223%: 100%|██████████| 572/572 [00:05<00:00, 110.59it/s]\n",
            "[20 / 50]   Val: Loss = 0.09917, Accuracy = 22.81737%: 100%|██████████| 13/13 [00:00<00:00, 84.99it/s]\n",
            "[21 / 50] Train: Loss = 0.04211, Accuracy = 33.52853%: 100%|██████████| 572/572 [00:05<00:00, 110.53it/s]\n",
            "[21 / 50]   Val: Loss = 0.09782, Accuracy = 21.89738%: 100%|██████████| 13/13 [00:00<00:00, 88.09it/s]\n",
            "[22 / 50] Train: Loss = 0.04025, Accuracy = 33.19654%: 100%|██████████| 572/572 [00:05<00:00, 110.37it/s]\n",
            "[22 / 50]   Val: Loss = 0.09910, Accuracy = 21.43265%: 100%|██████████| 13/13 [00:00<00:00, 85.81it/s]\n",
            "[23 / 50] Train: Loss = 0.03922, Accuracy = 33.25040%: 100%|██████████| 572/572 [00:05<00:00, 109.96it/s]\n",
            "[23 / 50]   Val: Loss = 0.10060, Accuracy = 22.24416%: 100%|██████████| 13/13 [00:00<00:00, 86.67it/s]\n",
            "[24 / 50] Train: Loss = 0.03779, Accuracy = 33.29558%: 100%|██████████| 572/572 [00:05<00:00, 110.74it/s]\n",
            "[24 / 50]   Val: Loss = 0.10045, Accuracy = 20.56831%: 100%|██████████| 13/13 [00:00<00:00, 81.83it/s]\n",
            "[25 / 50] Train: Loss = 0.03634, Accuracy = 32.90624%: 100%|██████████| 572/572 [00:05<00:00, 108.39it/s]\n",
            "[25 / 50]   Val: Loss = 0.10251, Accuracy = 20.80416%: 100%|██████████| 13/13 [00:00<00:00, 82.18it/s]\n",
            "[26 / 50] Train: Loss = 0.03514, Accuracy = 32.83617%: 100%|██████████| 572/572 [00:05<00:00, 109.86it/s]\n",
            "[26 / 50]   Val: Loss = 0.10181, Accuracy = 21.33163%: 100%|██████████| 13/13 [00:00<00:00, 83.59it/s]\n",
            "[27 / 50] Train: Loss = 0.03392, Accuracy = 32.42067%: 100%|██████████| 572/572 [00:05<00:00, 107.23it/s]\n",
            "[27 / 50]   Val: Loss = 0.10395, Accuracy = 21.23927%: 100%|██████████| 13/13 [00:00<00:00, 80.90it/s]\n",
            "[28 / 50] Train: Loss = 0.03296, Accuracy = 32.55878%: 100%|██████████| 572/572 [00:05<00:00, 107.08it/s]\n",
            "[28 / 50]   Val: Loss = 0.10489, Accuracy = 20.96676%: 100%|██████████| 13/13 [00:00<00:00, 83.91it/s]\n",
            "[29 / 50] Train: Loss = 0.03169, Accuracy = 32.68983%: 100%|██████████| 572/572 [00:05<00:00, 105.86it/s]\n",
            "[29 / 50]   Val: Loss = 0.10606, Accuracy = 19.94578%: 100%|██████████| 13/13 [00:00<00:00, 79.81it/s]\n",
            "[30 / 50] Train: Loss = 0.03057, Accuracy = 32.66307%: 100%|██████████| 572/572 [00:05<00:00, 105.43it/s]\n",
            "[30 / 50]   Val: Loss = 0.10716, Accuracy = 20.84244%: 100%|██████████| 13/13 [00:00<00:00, 80.06it/s]\n",
            "[31 / 50] Train: Loss = 0.02997, Accuracy = 32.61051%: 100%|██████████| 572/572 [00:05<00:00, 104.71it/s]\n",
            "[31 / 50]   Val: Loss = 0.10951, Accuracy = 21.91558%: 100%|██████████| 13/13 [00:00<00:00, 83.64it/s]\n",
            "[32 / 50] Train: Loss = 0.02866, Accuracy = 32.38608%: 100%|██████████| 572/572 [00:05<00:00, 107.66it/s]\n",
            "[32 / 50]   Val: Loss = 0.11084, Accuracy = 20.24538%: 100%|██████████| 13/13 [00:00<00:00, 81.75it/s]\n",
            "[33 / 50] Train: Loss = 0.02764, Accuracy = 32.52809%: 100%|██████████| 572/572 [00:05<00:00, 106.76it/s]\n",
            "[33 / 50]   Val: Loss = 0.11378, Accuracy = 22.55957%: 100%|██████████| 13/13 [00:00<00:00, 86.51it/s]\n",
            "[34 / 50] Train: Loss = 0.02664, Accuracy = 32.44191%: 100%|██████████| 572/572 [00:05<00:00, 107.62it/s]\n",
            "[34 / 50]   Val: Loss = 0.11300, Accuracy = 20.05882%: 100%|██████████| 13/13 [00:00<00:00, 83.92it/s]\n",
            "[35 / 50] Train: Loss = 0.02578, Accuracy = 32.41130%: 100%|██████████| 572/572 [00:05<00:00, 108.04it/s]\n",
            "[35 / 50]   Val: Loss = 0.11550, Accuracy = 19.86240%: 100%|██████████| 13/13 [00:00<00:00, 74.09it/s]\n",
            "[36 / 50] Train: Loss = 0.02509, Accuracy = 32.60231%: 100%|██████████| 572/572 [00:05<00:00, 108.75it/s]\n",
            "[36 / 50]   Val: Loss = 0.11821, Accuracy = 19.69686%: 100%|██████████| 13/13 [00:00<00:00, 81.33it/s]\n",
            "[37 / 50] Train: Loss = 0.02404, Accuracy = 32.38608%: 100%|██████████| 572/572 [00:05<00:00, 107.44it/s]\n",
            "[37 / 50]   Val: Loss = 0.12117, Accuracy = 20.22352%: 100%|██████████| 13/13 [00:00<00:00, 80.56it/s]\n",
            "[38 / 50] Train: Loss = 0.02331, Accuracy = 32.67279%: 100%|██████████| 572/572 [00:05<00:00, 107.41it/s]\n",
            "[38 / 50]   Val: Loss = 0.11976, Accuracy = 21.45025%: 100%|██████████| 13/13 [00:00<00:00, 84.94it/s]\n",
            "[39 / 50] Train: Loss = 0.02246, Accuracy = 32.49633%: 100%|██████████| 572/572 [00:05<00:00, 104.83it/s]\n",
            "[39 / 50]   Val: Loss = 0.12278, Accuracy = 20.82729%: 100%|██████████| 13/13 [00:00<00:00, 79.49it/s]\n",
            "[40 / 50] Train: Loss = 0.02159, Accuracy = 32.47751%: 100%|██████████| 572/572 [00:05<00:00, 105.30it/s]\n",
            "[40 / 50]   Val: Loss = 0.12518, Accuracy = 20.03229%: 100%|██████████| 13/13 [00:00<00:00, 82.69it/s]\n",
            "[41 / 50] Train: Loss = 0.02069, Accuracy = 32.65269%: 100%|██████████| 572/572 [00:05<00:00, 104.78it/s]\n",
            "[41 / 50]   Val: Loss = 0.12447, Accuracy = 20.37458%: 100%|██████████| 13/13 [00:00<00:00, 80.27it/s]\n",
            "[42 / 50] Train: Loss = 0.02026, Accuracy = 32.67525%: 100%|██████████| 572/572 [00:05<00:00, 104.79it/s]\n",
            "[42 / 50]   Val: Loss = 0.12415, Accuracy = 21.70861%: 100%|██████████| 13/13 [00:00<00:00, 86.56it/s]\n",
            "[43 / 50] Train: Loss = 0.01958, Accuracy = 32.48889%: 100%|██████████| 572/572 [00:05<00:00, 105.65it/s]\n",
            "[43 / 50]   Val: Loss = 0.12965, Accuracy = 21.03666%: 100%|██████████| 13/13 [00:00<00:00, 79.17it/s]\n",
            "[44 / 50] Train: Loss = 0.01866, Accuracy = 32.57178%: 100%|██████████| 572/572 [00:05<00:00, 106.22it/s]\n",
            "[44 / 50]   Val: Loss = 0.13206, Accuracy = 20.81884%: 100%|██████████| 13/13 [00:00<00:00, 81.73it/s]\n",
            "[45 / 50] Train: Loss = 0.01791, Accuracy = 32.68403%: 100%|██████████| 572/572 [00:05<00:00, 105.16it/s]\n",
            "[45 / 50]   Val: Loss = 0.13326, Accuracy = 21.88962%: 100%|██████████| 13/13 [00:00<00:00, 86.61it/s]\n",
            "[46 / 50] Train: Loss = 0.01743, Accuracy = 32.58149%: 100%|██████████| 572/572 [00:05<00:00, 107.33it/s]\n",
            "[46 / 50]   Val: Loss = 0.13422, Accuracy = 21.17939%: 100%|██████████| 13/13 [00:00<00:00, 82.31it/s]\n",
            "[47 / 50] Train: Loss = 0.01676, Accuracy = 32.53632%: 100%|██████████| 572/572 [00:05<00:00, 105.82it/s]\n",
            "[47 / 50]   Val: Loss = 0.13778, Accuracy = 21.94513%: 100%|██████████| 13/13 [00:00<00:00, 83.79it/s]\n",
            "[48 / 50] Train: Loss = 0.01606, Accuracy = 32.77889%: 100%|██████████| 572/572 [00:05<00:00, 104.51it/s]\n",
            "[48 / 50]   Val: Loss = 0.13790, Accuracy = 19.36343%: 100%|██████████| 13/13 [00:00<00:00, 80.65it/s]\n",
            "[49 / 50] Train: Loss = 0.01548, Accuracy = 32.47370%: 100%|██████████| 572/572 [00:05<00:00, 103.97it/s]\n",
            "[49 / 50]   Val: Loss = 0.14189, Accuracy = 21.53573%: 100%|██████████| 13/13 [00:00<00:00, 80.54it/s]\n",
            "[50 / 50] Train: Loss = 0.01538, Accuracy = 32.70605%: 100%|██████████| 572/572 [00:05<00:00, 105.07it/s]\n",
            "[50 / 50]   Val: Loss = 0.14300, Accuracy = 20.09856%: 100%|██████████| 13/13 [00:00<00:00, 80.94it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HPUuAPGhEGVR",
        "outputId": "048f7945-87e5-4351-9a13-9eca5d0b6bb6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 128
        }
      },
      "source": [
        "\n",
        "acc = 0\n",
        "counter = 0\n",
        "for i, (X_batch, y_batch) in enumerate(iterate_batches((X_test, y_test), 32)):\n",
        "    X, y = LongTensor(X_batch), LongTensor(y_batch)\n",
        "    logits = model(X)\n",
        "    _, indices = torch.max(logits, 2)\n",
        "    cur_acc = torch.mean(torch.tensor(y == indices, dtype=torch.float))\n",
        "    acc += cur_acc\n",
        "    counter = i\n",
        "print(\"Final acc: \", acc.item() / counter)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:14: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  \n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Final acc:  0.3670244664953859\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}